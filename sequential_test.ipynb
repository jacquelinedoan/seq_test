{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgwi2gRZnuyO28InsLHefT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacquelinedoan/seq_test/blob/main/sequential_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning for Optimal Alpha Spending Function in Sequential Hypothesis Testing**\n",
        "Rambling by Jacqueline.\n",
        "\n",
        "## Introduction\n",
        "Product experiements are commonly in the form of a $z$-test, comparing the 2 means of some KPI of both the control and test groups, say $\\mu_1$ and $\\mu_2$ respectively. The test is of course conducted after all data is collected. Furthermore,\n",
        "$$H_0: \\mu_1 = \\mu_2$$\n",
        "$$H_A: \\mu_1 < \\mu_2$$\n",
        "\n",
        "Given significance level $\\alpha$, if we find statistical significance, $H_0$ is rejected and we accept $H_A$; the new product is considered a success.\n",
        "*However, what happens when we do not find significance?*\n",
        "\n",
        "Say that we decided more data is needed and the data collection period is extended. Both the control and test groups grow in sample size. We then conduct the $z$-test a second time at some significance level. *Why don't we continue this process until we reached significance?*\n",
        "\n",
        "### *Sampling to reach a foregone conclusion*\n",
        "**Type I Error** (False Positive Error) is the risk of incorrectly rejecting a true $H_0$. In other words, we conclude the new version of the product is a success while it is not, risking shipping a product that does not have a positive effect on our customers.\n",
        "\n",
        "**Significance level $\\alpha$** is also the maximum probability of observing Type I Errors that the experimenter will accept in the long run.\n",
        "\n",
        "If we conduct the test once, the probability of not getting a FP error is $1-\\alpha$. If we conduct the tests above $k$ times, each at $\\alpha$, the change of observing at least one false positive grows\n",
        "$$P(\\geq 1 \\text{ FP }) = 1 - P(< 1 \\text{ FP }) = 1 - (1-\\alpha)^k$$\n",
        "\n",
        "*As $k \\to ∞$, just by chance, the probability you make at least one FP increases.*\n",
        "\n",
        "---\n",
        "**Formal Example**\n",
        "\n",
        "Let $X_1, X_2, \\dots$ i.i.d. from $N(\\mu, \\sigma^2)$ with known $\\sigma$. Consider a statistical tests of\n",
        "$$H_0: \\mu = 0$$\n",
        "$$H_1: \\mu \\neq 0$$\n",
        "Given fixed sample size $n$ and $\\alpha=0.05$, we reject $H_0$ iff\n",
        "$$\\left|\\sum_{i=1}^nX_i\\right| > 1.96\\sigma \\sqrt{n}$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The intergal (lol) relationship between $\\alpha$ and FP rate prompts an algorithm to compute it.\n",
        "\n",
        "\n",
        "---\n",
        "**Numerical Procedure to Compute Type I Error Rate**\n",
        "\n",
        "Let $X_1, X_2, \\dots$ i.i.d. $N(\\mu, \\sigma)$, and without loss of generality, let $\\sigma=1$. We conduct a sequence of hypothesis tests\n",
        "$$H_0: \\mu = 0$$\n",
        "$$H_1: \\mu \\neq 0$$\n",
        "and we label each test $k = 1,2, \\dots, K$. At each $k$, let $S_k = \\sum_i^k X_i$  and the boundary value be $b_k$. Sampling is terminated at step $k$ when we observe $S_k > b_k$ for the first time.\n",
        "\n",
        "The boundary values must satisfy the following\n",
        "$$ \\begin{align} &P(|S_k| > b_k \\text{ for some } k ) &= \\alpha \\\\\n",
        "\\implies &P(|S_k| < b_k \\text{ for all } k ) &= 1 - \\alpha\n",
        "\\end{align}\n",
        "$$\n",
        "Furthermore, $f_k$, the pdf of $S_k$ under $H_0$ satisfies the following recursive definition base on convolution\n",
        "$$f_k(s) = \\int_{-b_{k-1}}^{b_{k-1}}f_{k-1}(u)\\phi(s-u)du$$\n",
        "where $\\phi$ is the standard normal pdf. Let $k^*$ be the rv for when $|S_k|>b_k$ for the first time, and the probability of stopping at or before $k$ is\n",
        "$$\\begin{align}\n",
        "P_k &= P(k^* \\leq k)\\\\\n",
        "&= 1 - P(|S_1|\\leq b_1 \\dots |S_k|\\leq b_k) \\\\\n",
        "& = 1 - \\int_{-b_{k-1}}^{b_{k-1}}f_k(u)du\n",
        "\\end{align}\n",
        "$$\n",
        "The exit probability $P(k^* = k)$ is\n",
        "$$\\begin{align}\n",
        "P_k - P_{k-1} &= P(|S_1|\\leq b_1 \\dots |S_k|\\leq b_k, |S_k|>b_k) \\\\\n",
        "& = \\int_{-b_{k-1}}^{b_{k-1}}f_{k-1}(u){1-\\Phi(b_k-u) + \\Phi(-b_k -u)} du\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\Phi$ is the standard normal distribution function. The overall significance of the sequential procedure is\n",
        "$$\\alpha = 1 - \\int_{-b_{k-1}}^{b_{k-1}}f_K(u)du $$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "r76cia6uZNko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Literature\n",
        "Sequential tests are statistical tests to solve the problem above. Well-known techniques include group sequential tests, always valid inference, and corrected-alpha approach.\n",
        "\n",
        "In this notebook, I'm particularly interested in group sequential tests, particularly, finding the optimal **$\\alpha$ spending function**. This is a function that spreads $\\alpha$ over the sequence of $z$-tests, say $\\alpha_1, \\dots, \\alpha_k$. While there are many formulation for the function, I'm interested in framing the problem as an RL problem and viewing the function as an optimal policy. We view the problem set up stated by Lan and DeMets (1983) under the lense of product experimentation."
      ],
      "metadata": {
        "id": "Q9I6HwoFV33D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constrained Markov Decision Process (CMDP)\n",
        "Consider a Decision Maker (DM) who is conducting a sequence of 2-sample t-tests (unknown variance)(between control and test groups) with\n",
        "$$H_0: \\mu_C=\\mu_T $$\n",
        "$$H_A: \\mu_C < \\mu_T$$\n",
        "\n",
        "Framing the problem as a CMDP, our goal is to maximize power (probability of correct rejection) and the constraint is FPR $ = P(\\text{ Reject } H_0 | H_0 \\text{ true })\\leq \\alpha$. Or, formally,\n",
        "$$\\max_\\pi E_\\pi(\\text{Power}) \\text{ s.t. } E_\\pi(\\text{Type I Error rate}) \\leq \\alpha $$\n",
        "or equivalently,\n",
        "$$\\max_\\pi E_{H_A}(\\text{Utility}) \\text{ s.t. } E_{H_0}(1\\{\\text{Reject}\\}) \\leq \\alpha $$\n",
        "1. **State $s_k$**\n",
        "The state vector must contain all the information from the past that matters for future decisions for the policy to be Markovian. At each step $k$, the sufficient state vector is\n",
        "$$s_k = (t_k, \\nu_k, n_C, n_T, \\alpha_\\text{rem})$$\n",
        "where  \n",
        "$$t_k=\\frac{\\bar{X}_T - \\bar{X}_C}{\\sqrt{s^2_c/n_c + s^2_t/n_t} }$$\n",
        "The belief state $b_k$ = probability $H_A$ is true given data so far.\n",
        "\n",
        "\n",
        "2. **Action $a_t$**\n",
        "      *   Stop and Reject: Declare $H_A$\n",
        "      *   Continue Sampling and Propose an incremental spend $\\Delta\\alpha_k \\in [0, \\alpha_{rem}]$\n",
        "      *   Stop and Accept: Terminal state - Declare $H_0$\n",
        "3. **Step Reward**\n",
        "      * Stop and Reject and $H_A$ is true: $+R_{TP}$\n",
        "      * Stop and Reject and $H_0$ is true: $-R_{FP}$\n",
        "      * Stop and Accept and $H_A$ is true: $-R_{FN}$\n",
        "      * Continue Sampling: $-c$ for sampling cost\n",
        "\n"
      ],
      "metadata": {
        "id": "4VdyTo_gXLtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Primal-Dual (Lagrangian)**\n",
        "- Maximize $L(\\pi, \\lambda)$ using PPO\n",
        "- $\\lambda ← [\\lambda + η\\lambda(\\hat{C}(\\pi)-\\alpha)]_+$ where $\\hat{C}$ is the empirical Type-I estimate."
      ],
      "metadata": {
        "id": "b5nokAKczPyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "alpha_spend_t_ppo.py\n",
        "\n",
        "PPO + primal-dual scaffold to learn an alpha-spending function for sequential two-sample t-tests.\n",
        "- Uses Monte-Carlo conditional inversion to map proposed Δα -> t-boundary (cached).\n",
        "- Policy outputs a fraction of remaining alpha to spend and a futility-stop probability.\n",
        "- Episode-level cost: indicator of false positive (reject under H0). Constraint enforced with Lagrange multiplier λ.\n",
        "- This is a scaffold for experiments — increase MC sizes and training iterations for production.\n",
        "\n",
        "Run: python alpha_spend_t_ppo.py\n",
        "\"\"\"\n",
        "\n",
        "import os, math, time, pickle, random\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simple RNG seeding for reproducibility\n",
        "\n",
        "SEED = 1234\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "# -------------------- Config --------------------\n",
        "DEFAULT_CONFIG = {\n",
        "    \"alpha\": 0.05,\n",
        "    \"max_looks\": 6,\n",
        "    \"batch_per_look\": 5,   # number of samples per group added at each look\n",
        "    \"mc_precompute_n\": 2000,\n",
        "    \"mc_lookup_n\": 800,\n",
        "    \"cache_path\": \"/mnt/data/alpha_t_cache.pkl\",\n",
        "    \"train_iters\": 200,\n",
        "    \"batch_episodes\": 64,\n",
        "    \"lr\": 3e-4,\n",
        "    \"lambda_lr\": 1e-3,\n",
        "    \"ppo_clip\": 0.2,\n",
        "    \"device\": \"cpu\",\n",
        "    \"grid_delta_alpha\": np.linspace(1e-4, 0.2, 30).tolist(),\n",
        "    \"info_grid\": list(range(1,7)),\n",
        "    # simulation specifics\n",
        "    \"mu0\": 0.0,    # control mean\n",
        "    \"delta\": 0.5,  # true effect mean shift under H1\n",
        "    \"sigma\": 1.0,  # assumed common sd for data generation\n",
        "    \"init_n_per_group\": 2,  # initial samples per group at look 1\n",
        "}\n",
        "\n",
        "EpisodeResult = namedtuple(\"EpisodeResult\", [\"trajectory\", \"rejected\", \"true_H\", \"n_looks\", \"typeI_flag\"])\n",
        "\n",
        "# -------------------- Boundary cache / MC solver --------------------\n",
        "class BoundaryCache:\n",
        "    def __init__(self, path=None):\n",
        "        self.cache = {}\n",
        "        self.path = path\n",
        "        if path and os.path.exists(path):\n",
        "            try:\n",
        "                with open(path, \"rb\") as f:\n",
        "                    self.cache = pickle.load(f)\n",
        "                print(f\"Loaded cache from {path} ({len(self.cache)} entries)\")\n",
        "            except Exception as e:\n",
        "                print(\"Failed loading cache:\", e)\n",
        "                self.cache = {}\n",
        "    def save(self):\n",
        "        if not self.path: return\n",
        "        try:\n",
        "            with open(self.path, \"wb\") as f:\n",
        "                pickle.dump(self.cache, f)\n",
        "            print(f\"Saved cache to {self.path} ({len(self.cache)} entries)\")\n",
        "        except Exception as e:\n",
        "            print(\"Failed saving cache:\", e)\n",
        "    def key(self, prev_bounds, info_idx, delta_alpha):\n",
        "        k = sum(1 for b in prev_bounds if b is not None and np.isfinite(b))\n",
        "        m = 0 if len(prev_bounds)==0 else int(np.floor(min(prev_bounds)*10.0))\n",
        "        da_q = round(float(delta_alpha), 5)\n",
        "        return (k, m, info_idx, da_q)\n",
        "    def get(self, prev_bounds, info_idx, delta_alpha):\n",
        "        return self.cache.get(self.key(prev_bounds, info_idx, delta_alpha), None)\n",
        "    def set(self, prev_bounds, info_idx, delta_alpha, h):\n",
        "        self.cache[self.key(prev_bounds, info_idx, delta_alpha)] = float(h)\n",
        "\n",
        "def compute_t_stat_increment(n_new, mu_control, mu_treat, sigma, rng):\n",
        "    \"\"\"\n",
        "    Simulate new data for both groups (n_new per group) and return the two-sample t-statistic (Welch style)\n",
        "    for the updated pooled sample (we return the t-stat computed on the incremental combined sample only).\n",
        "    For MC boundary computation under H0 we will set mu_control == mu_treat.\n",
        "    \"\"\"\n",
        "    # generate samples\n",
        "    x_c = rng.normal(loc=mu_control, scale=sigma, size=n_new)\n",
        "    x_t = rng.normal(loc=mu_treat, scale=sigma, size=n_new)\n",
        "    # compute sample means and variances\n",
        "    n1 = len(x_c); n2 = len(x_t)\n",
        "    m1 = x_c.mean(); m2 = x_t.mean()\n",
        "    s1 = x_c.var(ddof=1) if n1>1 else 0.0\n",
        "    s2 = x_t.var(ddof=1) if n2>1 else 0.0\n",
        "    # Welch t for these incremental batches (not cumulative)\n",
        "    denom = math.sqrt(s1/n1 + s2/n2) if (s1>0 or s2>0) else 1e-8\n",
        "    t = (m2 - m1) / denom\n",
        "    # approximate df (Welch-Satterthwaite) for incremental\n",
        "    num = (s1/n1 + s2/n2)**2\n",
        "    den = 0.0\n",
        "    if n1>1:\n",
        "        den += (s1/n1)**2 / (n1 - 1)\n",
        "    if n2>1:\n",
        "        den += (s2/n2)**2 / (n2 - 1)\n",
        "    df = num / den if den>0 else max(n1+n2-2, 1.0)\n",
        "    return float(t), float(df)\n",
        "\n",
        "def compute_boundary_conditional(prev_bounds, info_idx, delta_alpha, n_mc=2000, rng=None, config=None):\n",
        "    \"\"\"\n",
        "    Simulate t-statistics under H0 conditional on surviving prev_bounds and find h such that\n",
        "    fraction crossing >= h equals delta_alpha.\n",
        "    Simplifying assumption: treat the t-statistic at current look as approximately Normal(0,1) marginally,\n",
        "    but we will simulate via small-batch t computations to get realistic tails.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.RandomState()\n",
        "    if delta_alpha <= 1e-12:\n",
        "        return 10.0\n",
        "    if delta_alpha >= 1 - 1e-12:\n",
        "        return -10.0\n",
        "    mu0 = config.get(\"mu0\", 0.0)\n",
        "    sigma = config.get(\"sigma\", 1.0)\n",
        "    n_new = config.get(\"batch_per_look\", 1)\n",
        "    samples = []\n",
        "    batch = max(2048, n_mc)\n",
        "    # If no prev boundaries, sample t directly under H0\n",
        "    if not prev_bounds:\n",
        "        # simulate n_mc incremental t-statistics under H0\n",
        "        draws = rng.normal(loc=0.0, scale=1.0, size=n_mc)\n",
        "        # approximate by scaling: treat draws as t-statistics (simple)\n",
        "        samples = draws\n",
        "    else:\n",
        "        # survival conditioning: we approximate by rejection sampling on previous min boundary\n",
        "        thr = min(prev_bounds)\n",
        "        survivors = []\n",
        "        tries = 0\n",
        "        while len(survivors) < n_mc and tries < 200:\n",
        "            draws = rng.normal(size=batch)\n",
        "            survivors.extend(draws[draws <= thr].tolist())\n",
        "            tries += 1\n",
        "            if tries>100 and len(survivors)==0:\n",
        "                # fallback: sample direct normals\n",
        "                survivors = list(np.random.normal(size=n_mc))\n",
        "                break\n",
        "        samples = np.array(survivors[:n_mc])\n",
        "    # empirical quantile\n",
        "    h = float(np.quantile(samples, 1.0 - float(delta_alpha)))\n",
        "    return h\n",
        "\n",
        "def precompute_boundaries(cache, config):\n",
        "    deltas = config[\"grid_delta_alpha\"]\n",
        "    info_grid = config[\"info_grid\"]\n",
        "    max_prev = config[\"max_looks\"] - 1\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    total = len(deltas) * len(info_grid) * (max_prev + 1)\n",
        "    i = 0\n",
        "    t0 = time.time()\n",
        "    for k in range(max_prev + 1):\n",
        "        prev_bounds = [2.5] * k if k>0 else []\n",
        "        for info_idx in info_grid:\n",
        "            for da in deltas:\n",
        "                key = cache.key(prev_bounds, info_idx, da)\n",
        "                if key in cache.cache:\n",
        "                    continue\n",
        "                h = compute_boundary_conditional(prev_bounds, info_idx, da, n_mc=config[\"mc_precompute_n\"], rng=rng, config=config)\n",
        "                cache.cache[key] = float(h)\n",
        "                i += 1\n",
        "                if i % 200 == 0:\n",
        "                    print(f\"Precompute {i}/{total} elapsed {time.time()-t0:.1f}s\")\n",
        "    cache.save()\n",
        "\n",
        "def lookup_boundary(cache, prev_bounds, info_idx, delta_alpha, config):\n",
        "    hit = cache.get(prev_bounds, info_idx, delta_alpha)\n",
        "    if hit is not None:\n",
        "        return hit\n",
        "    # nearest-key heuristic\n",
        "    k = sum(1 for b in prev_bounds if b is not None and np.isfinite(b))\n",
        "    candidates = []\n",
        "    for key, val in cache.cache.items():\n",
        "        if key[0]==k and key[2]==info_idx:\n",
        "            candidates.append((abs(key[3]-round(float(delta_alpha),5)), val))\n",
        "    if candidates:\n",
        "        candidates.sort(key=lambda x: x[0])\n",
        "        return float(candidates[0][1])\n",
        "    # fallback MC\n",
        "    h = compute_boundary_conditional(prev_bounds, info_idx, delta_alpha, n_mc=config[\"mc_lookup_n\"], rng=np.random.RandomState(), config=config)\n",
        "    cache.set(prev_bounds, info_idx, delta_alpha, h)\n",
        "    return h\n",
        "\n",
        "# -------------------- Simulator (sequential two-sample t-test) --------------------\n",
        "def run_episode(policy, cache, config, rng=None, quick_approx=False):\n",
        "    if rng is None:\n",
        "        rng = np.random.RandomState()\n",
        "    max_looks = config[\"max_looks\"]\n",
        "    batch_per_look = config[\"batch_per_look\"]\n",
        "    alpha = config[\"alpha\"]\n",
        "    mu0 = config[\"mu0\"]\n",
        "    delta = config[\"delta\"]\n",
        "    sigma = config[\"sigma\"]\n",
        "    # sample true hypothesis\n",
        "    H1 = (rng.rand() < 0.5)\n",
        "    mu_t = mu0 + (delta if H1 else 0.0)\n",
        "    # initialize samples (small initial burn-in)\n",
        "    n_c = config.get(\"init_n_per_group\", 2)\n",
        "    n_t = config.get(\"init_n_per_group\", 2)\n",
        "    # draw initial data\n",
        "    data_c = list(rng.normal(loc=mu0, scale=sigma, size=n_c))\n",
        "    data_t = list(rng.normal(loc=mu_t, scale=sigma, size=n_t))\n",
        "    mean_c = np.mean(data_c); mean_t = np.mean(data_t)\n",
        "    var_c = np.var(data_c, ddof=1) if n_c>1 else 0.0\n",
        "    var_t = np.var(data_t, ddof=1) if n_t>1 else 0.0\n",
        "    alpha_rem = alpha\n",
        "    prev_boundaries = []\n",
        "    trajectory = []\n",
        "    rejected = False\n",
        "    typeI_flag = False\n",
        "    Z = 0.0  # we'll keep t-stat in variable name t_stat\n",
        "    for look in range(1, max_looks+1):\n",
        "        # compute t-stat (Welch)\n",
        "        denom = math.sqrt((var_c / n_c) + (var_t / n_t)) if (var_c>0 or var_t>0) else 1e-8\n",
        "        t_stat = (mean_t - mean_c) / denom\n",
        "        # approximate df\n",
        "        num = (var_c/n_c + var_t/n_t)**2\n",
        "        den = 0.0\n",
        "        if n_c>1:\n",
        "            den += (var_c/n_c)**2 / (n_c - 1)\n",
        "        if n_t>1:\n",
        "            den += (var_t/n_t)**2 / (n_t - 1)\n",
        "        nu = num/den if den>0 else max(n_c+n_t-2, 1.0)\n",
        "        state = np.array([t_stat, nu, n_c, n_t, alpha_rem], dtype=np.float32)\n",
        "        action, logp, value = policy.act(state)\n",
        "        delta_alpha = float(action['delta_alpha_frac']) * alpha_rem\n",
        "        delta_alpha = max(0.0, min(alpha_rem, delta_alpha))\n",
        "        info_idx = look\n",
        "        # map to boundary\n",
        "        if delta_alpha > 0:\n",
        "            if quick_approx:\n",
        "                # crude: use normal quantile\n",
        "                h_t = float(np.quantile(np.random.normal(size=10000), 1.0 - delta_alpha))\n",
        "            else:\n",
        "                h_t = lookup_boundary(cache, prev_boundaries, info_idx, delta_alpha, config)\n",
        "        else:\n",
        "            h_t = 10.0\n",
        "        # sample next batch\n",
        "        x_c = rng.normal(loc=mu0, scale=sigma, size=batch_per_look)\n",
        "        x_t = rng.normal(loc=mu_t, scale=sigma, size=batch_per_look)\n",
        "        # update data summaries incrementally\n",
        "        # append then recompute mean/var for simplicity (n small)\n",
        "        data_c.extend(x_c.tolist())\n",
        "        data_t.extend(x_t.tolist())\n",
        "        n_c = len(data_c); n_t = len(data_t)\n",
        "        mean_c = np.mean(data_c); mean_t = np.mean(data_t)\n",
        "        var_c = np.var(data_c, ddof=1) if n_c>1 else 0.0\n",
        "        var_t = np.var(data_t, ddof=1) if n_t>1 else 0.0\n",
        "        # recompute t_stat after including new batch\n",
        "        denom = math.sqrt((var_c / n_c) + (var_t / n_t)) if (var_c>0 or var_t>0) else 1e-8\n",
        "        t_stat = (mean_t - mean_c) / denom\n",
        "        # check crossing\n",
        "        if t_stat >= h_t:\n",
        "            rejected = True\n",
        "            trajectory.append((state, action, logp, value, 0.0))\n",
        "            break\n",
        "        # futility stop\n",
        "        stop_fut_prob = float(action['stop_fut_prob'])\n",
        "        if rng.rand() < stop_fut_prob:\n",
        "            rejected = False\n",
        "            break\n",
        "        # continue\n",
        "        alpha_rem -= delta_alpha\n",
        "        if delta_alpha > 0:\n",
        "            prev_boundaries.append(h_t)\n",
        "        trajectory.append((state, action, logp, value, -1.0))\n",
        "        if alpha_rem <= 1e-12:\n",
        "            break\n",
        "    if rejected and not H1:\n",
        "        typeI_flag = True\n",
        "    return EpisodeResult(trajectory=trajectory, rejected=rejected, true_H=int(H1), n_looks=len(trajectory)+1, typeI_flag=typeI_flag)\n",
        "\n",
        "# -------------------- PPO policy --------------------\n",
        "class PPOPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim=5, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(obs_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden), nn.ReLU())\n",
        "        self.stop_fut_head = nn.Linear(hidden, 1)\n",
        "        self.delta_head = nn.Linear(hidden, 1)\n",
        "        self.value_head = nn.Linear(hidden, 1)\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        stop_logit = self.stop_fut_head(h).squeeze(-1)\n",
        "        delta_raw = torch.sigmoid(self.delta_head(h)).squeeze(-1)\n",
        "        value = self.value_head(h).squeeze(-1)\n",
        "        stop_prob = torch.sigmoid(stop_logit)\n",
        "        return stop_prob, delta_raw, value\n",
        "    def act(self, state):\n",
        "        st = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            stop_prob, delta_raw, val = self.forward(st)\n",
        "        stop_prob = float(stop_prob.item())\n",
        "        delta_raw = float(delta_raw.item())\n",
        "        stop_fut = 1.0 if np.random.rand() < stop_prob else 0.0\n",
        "        logp = math.log(stop_prob + 1e-8) if stop_fut==1.0 else math.log(1.0 - stop_prob + 1e-8)\n",
        "        action = {'stop_fut_prob': stop_prob, 'delta_alpha_frac': delta_raw}\n",
        "        return action, logp, float(val.item())\n",
        "\n",
        "# -------------------- Training loop (primal-dual PPO-like) --------------------\n",
        "def train_ppo(config, quick=False):\n",
        "    device = torch.device(config.get(\"device\",\"cpu\"))\n",
        "    policy = PPOPolicy().to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=config[\"lr\"])\n",
        "    cache = BoundaryCache(path=config.get(\"cache_path\", None))\n",
        "    # Precompute grid (unless quick)\n",
        "    if len(cache.cache)==0 and not quick:\n",
        "        print(\"Precomputing boundary cache (this may take a while)...\")\n",
        "        precompute_boundaries(cache, config)\n",
        "    lambda_dual = 1.0\n",
        "    for it in range(1, config[\"train_iters\"]+1):\n",
        "        episodes = []\n",
        "        typeI_count = 0\n",
        "        for e in range(config[\"batch_episodes\"]):\n",
        "            ep = run_episode(policy, cache, config, rng=np.random.RandomState(), quick_approx=quick)\n",
        "            episodes.append(ep)\n",
        "            if ep.typeI_flag:\n",
        "                typeI_count += 1\n",
        "        typeI_hat = typeI_count / len(episodes)\n",
        "        lambda_dual = max(0.0, lambda_dual + config[\"lambda_lr\"] * (typeI_hat - config[\"alpha\"]))\n",
        "        # prepare training batch\n",
        "        states = []\n",
        "        old_logps = []\n",
        "        returns = []\n",
        "        for ep in episodes:\n",
        "            # episodic utility: +1 for TP, -1 for FN, -1 - lambda for FP under H0\n",
        "            if ep.rejected and ep.true_H==1:\n",
        "                u = 1.0\n",
        "            elif not ep.rejected and ep.true_H==1:\n",
        "                u = -1.0\n",
        "            elif ep.rejected and ep.true_H==0:\n",
        "                u = -1.0 - lambda_dual\n",
        "            else:\n",
        "                u = 0.0\n",
        "            if len(ep.trajectory)>0:\n",
        "                s0, a0, logp0, v0, r0 = ep.trajectory[0]\n",
        "                states.append(s0)\n",
        "                old_logps.append(logp0)\n",
        "                returns.append(u)\n",
        "            else:\n",
        "                states.append(np.array([0.0,0.0, config.get(\"init_n_per_group\",2), config.get(\"init_n_per_group\",2), config[\"alpha\"]], dtype=np.float32))\n",
        "                old_logps.append(math.log(0.5))\n",
        "                returns.append(u)\n",
        "        states_t = torch.tensor(np.stack(states,axis=0), dtype=torch.float32).to(device)\n",
        "        returns_t = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        old_logp_t = torch.tensor(old_logps, dtype=torch.float32).to(device)\n",
        "        stop_prob_t, delta_t, val_t = policy.forward(states_t)\n",
        "        advantages = (returns_t - val_t.detach()).detach()\n",
        "        # actor loss (stop_prob only, approximate)\n",
        "        sampled_bit = (old_logp_t > math.log(0.5)).float()\n",
        "        curr_logp = sampled_bit * torch.log(stop_prob_t + 1e-8) + (1.0-sampled_bit) * torch.log(1.0 - stop_prob_t + 1e-8)\n",
        "        ratios = torch.exp(curr_logp - old_logp_t)\n",
        "        surr1 = ratios * advantages\n",
        "        surr2 = torch.clamp(ratios, 1.0-config[\"ppo_clip\"], 1.0+config[\"ppo_clip\"]) * advantages\n",
        "        actor_loss = -torch.mean(torch.min(surr1, surr2))\n",
        "        delta_loss = -torch.mean(advantages * delta_t)\n",
        "        critic_loss = torch.mean((val_t - returns_t)**2)\n",
        "        loss = actor_loss + 0.5 * critic_loss + 0.1 * delta_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if it % 10 == 0 or it==1:\n",
        "            avg_util = float(np.mean(returns))\n",
        "            print(f\"Iter {it:04d} | typeI_hat={typeI_hat:.3f} | lambda={lambda_dual:.3f} | avg_util={avg_util:.3f}\")\n",
        "    cache.save()\n",
        "    return policy, cache, lambda_dual\n",
        "\n",
        "# -------------------- Main (smoke run) --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = DEFAULT_CONFIG.copy()\n",
        "    cfg[\"train_iters\"] = 60\n",
        "    cfg[\"batch_episodes\"] = 48\n",
        "    cfg[\"mc_precompute_n\"] = 400\n",
        "    cfg[\"mc_lookup_n\"] = 200\n",
        "    cfg[\"cache_path\"] = \"/content/cache/alpha_t_cache_smoke.pkl\"\n",
        "    policy, cache, lambda_final = train_ppo(cfg, quick=True)\n",
        "    print(\"Done. final lambda:\", lambda_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L_opiYM6j67",
        "outputId": "e182fc16-a5de-401c-e60d-ac10aef90f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0001 | typeI_hat=0.000 | lambda=1.000 | avg_util=-0.375\n",
            "Iter 0010 | typeI_hat=0.021 | lambda=1.000 | avg_util=-0.396\n",
            "Iter 0020 | typeI_hat=0.021 | lambda=0.999 | avg_util=-0.375\n",
            "Iter 0030 | typeI_hat=0.021 | lambda=0.999 | avg_util=-0.417\n",
            "Iter 0040 | typeI_hat=0.021 | lambda=0.999 | avg_util=-0.167\n",
            "Iter 0050 | typeI_hat=0.021 | lambda=0.999 | avg_util=-0.292\n",
            "Iter 0060 | typeI_hat=0.062 | lambda=0.998 | avg_util=-0.354\n",
            "Saved cache to /content/cache/alpha_t_cache_smoke.pkl (0 entries)\n",
            "Done. final lambda: 0.9984166666666667\n"
          ]
        }
      ]
    }
  ]
}